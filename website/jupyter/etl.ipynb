{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a5eaad",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load (ETL)\n",
    "---\n",
    "The purpose of this Jupyter Notebook is to extract data and storing it in a SQLite database within this package. The codes below show the steps in this process:\n",
    "- Data pulling <i>['Starbucks', 'Dunkin' Donuts', 'Think Coffee', 'Joe Coffee', 'Gregorys Coffee', 'Birch Coffee']</i> located around NYC\n",
    "    - <b>NYC location -</b> longitude: -73.99429321289062, latitude: 40.70544486444615\n",
    "    - Includes these areas: Brooklyn, the Bronx, Manhattan, Queens and Staten Island\n",
    "    - Using search radius of 40000 (meters) or ~25 miles\n",
    "- ETL - for only needed data that will be used to store in the database\n",
    "- Configure database\n",
    "    - Delete (if exists) and create a new database coffee_chains.sqlite and the table\n",
    "    - Load data into the database\n",
    "\n",
    "#### Note that this is not subjected to only coffee finding but can be any other search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3afa587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from api_key import api_key\n",
    "from jsonschema import validate\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Yelp API constants\n",
    "API_HOST = 'https://api.yelp.com/v3/businesses/search'\n",
    "HEADERS = {\n",
    "    'Authorization': 'bearer %s' % api_key\n",
    "}\n",
    "\n",
    "# Schema for comparing in checking before extraction\n",
    "schema = {\n",
    "    'alias': '',\n",
    "    'categories': [],\n",
    "    'coordinates': {},\n",
    "    'display_phone': '',\n",
    "    'distance': 0.00,\n",
    "    'id': 'string',\n",
    "    'image_url': '',\n",
    "    'is_closed': True,\n",
    "    'location': {},\n",
    "    'name': '',\n",
    "    'phone': '',\n",
    "    'price': '',\n",
    "    'rating': 0.0,\n",
    "    'review_count': 0,\n",
    "    'transaction': [],\n",
    "    'url': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcc1e3",
   "metadata": {},
   "source": [
    "### Custom functions for the ETL steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple request function for bussiness search endpoint from Yelp API\n",
    "def request(term = '', loc = '', offsets = 200, rad = 10000):\n",
    "    data = []\n",
    "    \n",
    "    for offset in range(0, offsets, 50):\n",
    "        params = {\n",
    "            'term': term.replace(' ', '+'),\n",
    "            'location': loc.replace(' ', '+'),\n",
    "            'limit': 50,\n",
    "            'offset': offset,\n",
    "            'radius': rad\n",
    "        }\n",
    "\n",
    "        # Send the request\n",
    "        response = requests.get(API_HOST, headers = HEADERS, params = params)\n",
    "\n",
    "        # Verify the response and return None if error returned else return the json data\n",
    "        if response.status_code == 200:            \n",
    "            data += response.json()['businesses']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to verify the predefined schema on what we should be expecting before extracting\n",
    "def verify_schema(data = None):\n",
    "    \n",
    "    # Verify the object entered before extraction\n",
    "    try:\n",
    "        validate(instance=data, schema=schema)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Return the json to DF for cleanining\n",
    "def json_to_dataframe(data = None):\n",
    "    try:\n",
    "        if not verify_schema(data[0]):\n",
    "            return pd.DataFrame({'error': [\"{'error': 'SCHEMA VALIDATION ERROR'}\"]})\n",
    "        else:\n",
    "            return pd.DataFrame(data)\n",
    "    except TypeError:\n",
    "        return pd.DataFrame({'error': [\"{'error': 'OBJECT INPUT ERROR'}\"]})\n",
    "\n",
    "# Function to extract the id, name, price, rating, review_count, location (address 1, address 2, address 3, city, \n",
    "# state, zip_code), coordinates (latitude and longtitude), and phone into a DataFrame\n",
    "def cleaned_yelp_dataframe(df, name, filter_by_arr = ['']):\n",
    "    # Create a copy of the df to work with\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Sometimes random result return not matching the criteria, filter those out by the name\n",
    "    key_search = [name.lower()]\n",
    "    clean_df['name'] = clean_df.name.apply(lambda x: x.lower())\n",
    "    clean_df = clean_df.loc[clean_df['name'].isin(key_search)]\n",
    "    \n",
    "    # Normalizing the coordinates and location columns with nested dictionary\n",
    "    clean_df[['latitude', 'longitude']] = pd.json_normalize(clean_df['coordinates'])\n",
    "    clean_df[[\n",
    "        'address1', \n",
    "        'address2', \n",
    "        'address3', \n",
    "        'city', \n",
    "        'zip', \n",
    "        'country', \n",
    "        'state', \n",
    "        'display_address'\n",
    "    ]] = pd.json_normalize(clean_df['location'])\n",
    "    \n",
    "    # Drop off the columns no longer needed\n",
    "    clean_df = clean_df.drop(columns = [\n",
    "        'alias', 'is_closed', 'categories', 'coordinates', 'transactions', 'location',\n",
    "        'phone', 'distance', 'display_address'        \n",
    "    ])\n",
    "    \n",
    "    # Add price point column, fill na, and convert to int\n",
    "    clean_df['price'] = clean_df['price'].fillna('')\n",
    "    clean_df['price_point'] = clean_df['price'].str.len()\n",
    "    clean_df['price_point'] = clean_df['price_point'].fillna(0)\n",
    "    clean_df['price_point'] = clean_df['price_point'].astype('int')\n",
    "    \n",
    "    # Reorganize the df for easy viewing\n",
    "    clean_df = clean_df[[\n",
    "        'id', 'name', 'review_count', 'rating', 'price', 'price_point', 'display_phone', 'url', 'image_url', 'address1', \n",
    "        'address2', 'address3', 'city', 'state', 'zip', 'country', 'latitude', 'longitude'\n",
    "    ]]\n",
    "    \n",
    "    # Filter out just stores found within NYC\n",
    "    key_search = filter_by\n",
    "    clean_df = clean_df.loc[clean_df['city'].isin(key_search)]\n",
    "    \n",
    "    # Replace any \"None\" values from address2 and address3 to \"\"\n",
    "    clean_df['address2'] = clean_df['address2'].fillna('')\n",
    "    clean_df['address3'] = clean_df['address3'].fillna('')\n",
    "    \n",
    "    # Proper casing for the name and city\n",
    "    clean_df['name'] = clean_df['name'].str.title()\n",
    "\n",
    "    # Take only rows that do not have nan for coordinates\n",
    "    clean_df = clean_df[clean_df['latitude'].notna()]\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8be0d",
   "metadata": {},
   "source": [
    "### Perform the ETL into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using for loop to pull the requests, clean, and then combine to export out into one single json file and add to database\n",
    "search_locations = ['nyc', 'bronx, NY', 'queens, NY', 'staten island, NY', 'brooklyn, NY']\n",
    "search_terms = ['Starbucks', 'Dunkin\\' Donuts', 'Tim Hortons', 'Think Coffee', 'Joe Coffee', 'Gregorys Coffee', 'Birch Coffee']\n",
    "filter_by = ['New York', 'Brooklyn', 'Bronx', 'Manhattan', 'Queens', 'Staten Island']\n",
    "dfs = []\n",
    "\n",
    "# Loop through to append the output cleaned df of each coffee chain for the merging using the defined functions from above\n",
    "for loc in search_locations:\n",
    "    for term in search_terms:\n",
    "        response_data = request(term, loc, offsets = 200)\n",
    "        response_data = json_to_dataframe(response_data)\n",
    "        cleaned_response_data = cleaned_yelp_dataframe(response_data, term, filter_by)\n",
    "        dfs.append(cleaned_response_data)\n",
    "\n",
    "# Now perform the merge\n",
    "df_merged = pd.concat(dfs)\n",
    "# Count how many before dropping duplicates and count duplicates\n",
    "print(df_merged.shape[0])\n",
    "print(df_merged.groupby(df_merged.columns.tolist(),as_index=False).size())\n",
    "\n",
    "# Drop any duplicates\n",
    "df_merged = df_merged.drop_duplicates()\n",
    "# Count how many after dropping duplicates\n",
    "print(df_merged.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check/reveview the merged DF for reviewing\n",
    "df_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the dtypes for the DF\n",
    "df_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export out into csv and json in addition to adding it to a SQLite database\n",
    "df_merged.to_csv('../static/dataset/merged.csv', index = False)\n",
    "df_merged.to_json('../static/dataset/merged.json', orient = 'records', indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies for handling the database\n",
    "from os import path, remove\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the db path\n",
    "db_path = '../coffee_chains.sqlite'\n",
    "\n",
    "# Delete the existing database if it exists\n",
    "if path.exists(db_path):\n",
    "    remove(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the engine and connect the database\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the DF to the shop table created\n",
    "df_merged.to_sql(name = 'shops', con = engine, if_exists = 'replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af711d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to see if things got appended correctly\n",
    "session = Session(bind = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0007e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(text('SELECT * FROM shops')).fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42779939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close out of the session and engine\n",
    "session.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43a72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
